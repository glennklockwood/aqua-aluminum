---
title: Building and Running IO500
---

## Compiling

Step 1. Edit `Makefile` in the top level directory and edit `CC` and `CFLAGS` to
match the compiler and build parameters required

If you don't do this, you will get this error:

    mpicc -std=gnu99 -Wall -Wempty-body -Werror -Wstrict-prototypes -Werror=maybe-uninitialized -Warray-bounds -g3 -lefence -I./include/ -I./src/ -I./build/pfind/src/ -I./build/ior/src/ -DVERSION="\"io500-sc20_v3-6-gd25ea80d54c7\"" -c -o verifier.o src/verifier.c
    /bin/sh: mpicc: command not found
    make: *** [Makefile:59: verifier.o] Error 127

Step 2. Run `./prepare.sh` with `CC=` defined in the environment to match what
you put in the Makefile above:

    CC=cc ./prepare.sh

If you don't do this, you will get this error:

    Building parallel find;
    Using LZ4 for optimization
    ./compile.sh: line 22: mpicc: command not found

Edit `./prepare.sh` itself and edit the `build_ior` and related build functions
if needed.

## Running

The benchmark will benchmark whatever file system owns the `datadir` config
parameter in the ini file passed to it at launch time.  By default, this is

    [global]
    datadir = ./datafiles

which benchmarks a path relative to `$PWD`.  Similarly, it outputs its results
to whatever is given as the `resultdir` path.  By default, this is empty in
`config-minimal.ini` and is equivalent to

    [global]
    resultdir = ./results

Edit the to point to whatever path(s) you want, then do something like

    srun -N 4 \
         -n 64 \
         --qos regular \
         -C haswell \
         -t 30:00 \
         ./io500 config-minimal.ini

Alternatively submit it in batch mode, since it does run for a while (each test
runs for five minutes).

To integrate with a batch environment and dynamic mounts (e.g., a burst buffer)
you have to do a little bit of gymnastics since io500 only takes its config from
a preformatted file.

This is how I run against DataWarp:

```bash
#!/usr/bin/env bash
#SBATCH -N 4
#SBATCH -n 64
#SBATCH --qos regular
#SBATCH -C haswell
#SBATCH -t 30:00
#SBATCH -A nstaff
#DW jobdw type=scratch access_mode=striped capacity=20TiB

CONFIG_FILE="$SLURM_SUBMIT_DIR/config-$SLURM_JOBID.ini"

cat <<EOF > "$CONFIG_FILE"
[global]
datadir = $DW_JOB_STRIPED
EOF

srun "$SLURM_SUBMIT_DIR/io500" "$CONFIG_FILE"
```

Notes about the way the io500 binary works:

- It automatically scales the benchmark to match the nprocs it's given, and by
  default its paths are relative to whatever `$PWD` is.  So it inherits a lot
  from the execution environment.
- It uses stonewalling by default, so some of the input parameters may seem
  ridiculously large.  For example IOR easy is configured to write over 9 TB by
  default.

## Interpreting Results

The results directory contains timestamped output directories, one per run.  
This is pretty nice in that running the same io500 repeatedly does not wipe out
the results of previous runs.

In each directory are two important summary files:

- _result\_summary.txt_ - an easy, human-readable file with individual performance
  measurements and the IO500 score
- _result.txt_ - a machine-readable summary of results

The _result.txt_ file is nice, but it labels everything as "score" without
units.  For reference,

test                | score units |
--------------------|-------------|
ior-easy-write      | GiB/s       |
mdtest-easy-write   | kIOPS       |
ior-hard-write      | GiB/s       |
mdtest-hard-write   | kIOPS       |
find                | kIOPS       |
ior-easy-read       | GiB/s       |
mdtest-easy-stat    | kIOPS       |
ior-hard-read       | GiB/s       |
mdtest-hard-stat    | kIOPS       |
mdtest-easy-delete  | kIOPS       |
mdtest-hard-read    | kIOPS       |
mdtest-hard-delete  | kIOPS       |

The way the final IO500 score is calculated is first

1. Taking the geometric mean of the GiB/s scores
2. Taking the geometric mean of the kIOPS scores
3. Taking the geometric mean of #1 and #2

This is done rather than taking the geometric mean of all individual scores so
that metadata (kIOPS) are weighted equally with bandwidth (GiB/s).

Trying to attribute any intellectual value to the io500 score is problematic
though.  Consider the following points:

- Taking the geometric mean of GiB/s and kIOPS mixes units of measure in
  a nonsensical way and implies an equivalency between billions of bytes and
  thousands of operations: it establishes 1 GiB/s as being equivalently
  high-performance as 1 kIOPS.  This will undoubtedly change over time as IOPS
  rates outpace bandwidth rates, meaning that the io500 score will evolve to
  bias top-performers towards high metadata rates.
- It arbitrarily defines the ior-hard tests (performing 47 KiB I/Os) in terms of
  bandwidth instead of kIOPS.  At this size, some file systems and networks will
  be latency-limited, while others will be bandwidth-limited.  Again, as
  technology evolves, the intellectual meaning of defining ior-hard performance
  as GiB/s instead of kIOPS will slide around.

These two facts mean that these io500 scores will not be meaningfully comparable
across different generations of technology as iops bottlenecks turn into
bandwidth bottlenecks.
