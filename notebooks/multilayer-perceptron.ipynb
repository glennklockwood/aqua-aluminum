{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This walk-through was inspired by [Building Neural Networks with Python Code and Math in Detail  Part II](https://pub.towardsai.net/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1) and follows my walk-through of building a perceptron.  We will not rehash concepts covered previously and instead move quickly through the parts of building this neural network that follow the same pattern as building a perceptron.\n",
    "\n",
    "As with the perceptron guide, we first implementing our model using only numpy, then go on to implement it using PyTorch to understand what PyTorch is doing mathematically.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "As with our previous perceptron, we will be modeling a logical OR gate with two inputs ($x_1$ and $x_2$), one output ($f$).  Unlike before though, we will create a _multilayer perceptron_ where we chain two linear models together instead of using just one.\n",
    "\n",
    "First let's get our input loaded.  This time, we'll use a CSV as our input since this is a pretty common data format that we would expect to encounter in the real world.  We load it into a Pandas DataFrame, then peel off the last column to use as our ground-truth results.  The first two columns then become our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "Truth: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "input_csv = \"\"\"\n",
    "observation,input1,input2,output\n",
    "1,0,0,0\n",
    "2,0,1,1\n",
    "3,1,0,1\n",
    "4,1,1,1\n",
    "\"\"\"\n",
    "\n",
    "dataset = pandas.read_csv(io.StringIO(input_csv), index_col=\"observation\")\n",
    "inputs = dataset.iloc[:,:-1].to_numpy().astype('float32')\n",
    "ground_truth = dataset.iloc[:,-1].to_numpy().reshape(-1, 1).astype('float32')\n",
    "\n",
    "print(\"Inputs:\\n{}\".format(inputs))\n",
    "print(\"Truth: \\n{}\".format(ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we have the same _learning rate_ hyperparameter $R$, and we might as well use the same value.  We also have to decide how many iterations of training we wish to use, `NUM_ITERATIONS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "NUM_ITERATIONS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "\n",
    "We are implementing a neural network to model the same OR gate as our perceptron network before, so it has the same number of input features (2) and outputs (1) as before.  However, this time we are adding a _hidden layer_ in between with three weights for each input, and it can be of arbitrary \"size\"--that is to say, it can have as many features as we care to add, and the number of features govern how many weights are used to define it.\n",
    "\n",
    "There are some heuristics to deciding how to size a hidden layer discussed in [Building Neural Networks with Python Code and Math in Detail  Part II](https://pub.towardsai.net/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1), but it seems pretty arbitrary to me.  We choose a hidden layer with three features here just because."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEURONS_HIDDEN = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know \n",
    "\n",
    "1. the size of our inputs (our CSV contains four observations, and each observation has two input features $x_1$ and $x_2$),\n",
    "2. the size of our hidden layer (we arbitrarily defined it as having three features)\n",
    "3. the size of our output layer (it has a single value, either 1 or 0)\n",
    "\n",
    "we can draw the obligatory multilayer neural network graph that every tutorial online seems to have:\n",
    "\n",
    "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "\n",
    "I find these neural network diagrams very confusing because it's never clear to me what each node is supposed to represent (and in fact they represent two different things--sometimes the elements of an input vector, and sometimes a function with distinct inputs and outputs).  So I chose to label every node and edge of the above diagram to help map the visualization to what they represent mathematically.  To be explicit,\n",
    "\n",
    "- `x1` and `x2` are our input features for a single observation,\n",
    "- `v1` and `v2` are the weights for our hidden layer, and they are always multiplied by `x1` and `x2`, respectively, to establish our linear model `g`\n",
    "- `g` is the combination of our linear model (let's call it $\\hat{y}(\\mathbf{x})$) and its activation function ($g(\\hat{y}(\\mathbf{x}))$) that make up our hidden layer\n",
    "- `w1`, `w2`, and `w3` are the weights for our output layer\n",
    "- `f` is the combination of our linear model (which we'll call $y(\\mathbf{x})$) and its activation function ($f(y(\\mathbf{x}))$) that make up our output layer\n",
    "\n",
    "As with the perceptron we implemented in the previous page, we use a _loss function_ and _gradient descent_ to iterate until we find the values for the five weights across the two layers that produce a model that best represents our OR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with NumPy\n",
    "\n",
    "We are using the same linear model and activation function as we did before.  The linear model has the form\n",
    "\n",
    "$ y(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b$\n",
    "\n",
    "and the activation function is a sigmoid:\n",
    "\n",
    "$ \\displaystyle A(y) = \\frac{1}{1 + e^{-y}}$\n",
    "\n",
    "which are implemented in Python same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights, bias):\n",
    "    return numpy.dot(x, weights) + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to randomly assign weights again.  Each layer has its own weights that we must initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "[[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      " [3.02332573e-01 1.46755891e-01 9.23385948e-02]]\n",
      "\n",
      "Starting output layer weights:\n",
      "[[0.18626021]\n",
      " [0.34556073]\n",
      " [0.39676747]]\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1)\n",
    "weights_hidden = numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN)\n",
    "weights_output = numpy.random.rand(NUM_NEURONS_HIDDEN, 1)\n",
    "# bias = numpy.random.rand(1)[0]\n",
    "bias = 0.0\n",
    "\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(weights_hidden))\n",
    "print(\"Starting output layer weights:\\n{}\".format(weights_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are **not** using a bias this time around.\n",
    "\n",
    "The dimensionality of each layer in our neural network is a little more complicated in this multilayer case, which is why I drew the network out:\n",
    "\n",
    "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "\n",
    "but even this diagram doesn't keep the entirety of the network, its functions, and all the different matrix dimensions straight in my head.  So let's define all of the scalar components of our multilayer network explicitly to help us figure out the derivatives we need to perform gradient descent:\n",
    "\n",
    "1. input values: $\\mathbf{x}$\n",
    "2. hidden layer: $\\mathbf{\\hat{y}} = \\mathbf{x} \\cdot \\mathbf{v} $\n",
    "    0. $\\hat{y}_{11} = x_{11} v_{11} + x_{12} v_{21} $\n",
    "    0. $\\hat{y}_{21} = x_{21} v_{11} + x_{22} v_{21} $\n",
    "    0. ...\n",
    "    0. $\\hat{y}_{42} = x_{41} v_{12} + x_{42} v_{22} $\n",
    "    0. $\\hat{y}_{43} = x_{41} v_{13} + x_{42} v_{23} $\n",
    "3. activation of hidden layer: $ \\mathbf{g}(\\mathbf{\\hat{y}}) = \\frac{1}{1+e^{-\\mathbf{\\hat{y}}}} $\n",
    "4. output layer: $\\mathbf{y} = \\mathbf{g} \\cdot \\mathbf{w}$\n",
    "    0. $y_{11} = g_{11} w_{11} + g_{12} w_{21} + g_{13} w_{31} $\n",
    "    0. ...\n",
    "    0. $y_{41} = g_{41} w_{11} + g_{42} w_{21} + g_{43} w_{31} $\n",
    "5. activation of output layer: $ \\mathbf{f}(\\mathbf{y}) = \\frac{1}{1+e^{-y}} $\n",
    "\n",
    "So our neural network with its hidden layer is really expressed as\n",
    "\n",
    "$ \\mathbf{f}(\\mathbf{y}(\\mathbf{g}(\\mathbf{\\hat{y}}(\\mathbf{x})))) $\n",
    "\n",
    "Finally, we choose mean-squared error as our loss function:\n",
    "\n",
    "$ E(f) = \\frac{1}{N} \\displaystyle \\sum^{N} (f - f_{0})^2 $\n",
    "\n",
    "which has some subtlety since $f$ is really a matrix $\\mathbf{f}$, and therefore our loss function $E(f)$ yields a matrix $\\mathbf{E}$ whose elements $E_{ij}$ are defined as:\n",
    "\n",
    "$ E_{ij} = \\displaystyle \\frac{(f_{ij} - f_{0,ij})^2}{N} $\n",
    "\n",
    "where $N$ is the number of inputs used to train our neural network and result in the error calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Gradient descent in this multilayer case is very similar to gradient descent in the single perceptron case.  A couple of key points:\n",
    "\n",
    "1. Instead of the output layer taking its input from the input observations, it takes its input from the output of the hidden layer.  The hidden layer takes its input from the input observations\n",
    "2. Each layer has its own set of weights, so gradient descent is applied to each layer in sequence.\n",
    "3. The gradient is calculated starting at the output layer, and the chain rule is then applied to connect the gradients of the two layers.\n",
    "\n",
    "The last point is probably why the process of updating weights throughout a multilayer network is called _backpropagation_.  You have to start at the output and calculate gradients in the reverse direction of the network.\n",
    "\n",
    "#### Output layer gradient\n",
    "\n",
    "Calculating the gradient of the error (loss function) with respect to the weights begins at the output layer:\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial E}{\\partial w} =\n",
    "\\frac{\\partial E}{\\partial f} \\cdot\n",
    "\\frac{\\partial f}{\\partial y} \\cdot\n",
    "\\frac{\\partial y}{\\partial w}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial f} = \\frac{2}{N} ( f - f_0 ) $\n",
    "- $ \\frac{\\partial f}{\\partial y} = f (1 - f) $\n",
    "- $ \\frac{\\partial y}{\\partial w} = g$ (in the single perceptron case, $ \\frac{\\partial y}{\\partial w} = x $)\n",
    "\n",
    "This is the same as gradient descent in the single perceptron case _except_ that the inputs from this layer come from the hidden layer's output, $g(\\hat{y})$, instead of the observations $x$.\n",
    "\n",
    "#### Hidden layer gradient\n",
    "\n",
    "Calculating the gradient of the hidden layer's weights is the same as the output layer, except the inputs and outputs from this layer are different.  Again, we want to calculate the gradient of the error $E$ with respect to this layer's weights $v$:\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial E}{\\partial v} = \n",
    "\\frac{\\partial E}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial \\hat{y}}\n",
    "\\frac{\\partial \\hat{y}}{\\partial v} $\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial g} =\n",
    "           \\frac{\\partial E}{\\partial f} \\cdot\n",
    "           \\frac{\\partial f}{\\partial y} \\cdot\n",
    "           \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial g} =\n",
    "        \\frac{\\partial E}{\\partial y} \\cdot\n",
    "        \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial y} =\n",
    "        \\frac{\\partial E}{\\partial f} \\cdot\n",
    "        \\frac{\\partial f}{\\partial y}$\n",
    "- $ \\frac{\\partial g}{\\partial \\hat{y}} = g (1 - g)$\n",
    "- $ \\frac{\\partial \\hat{y}}{\\partial v} = x$\n",
    "\n",
    "We rely on chain-rule expanding $\\frac{\\partial E}{\\partial g}$ so we can solve for it in terms of the output layer's partial derivatives.  This is where the two layers' terms connect to each other, and it is how the gradient we calculated for the output layer help us calculate the gradient for this hidden layer.\n",
    "\n",
    "The only other differences is that this hidden layer takes its input from the observations provided at the beginning of training, $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651662e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350530e-02\n",
      "error at step  9000: 6.205274e-02\n",
      "Final weights:\n",
      "  hidden: [ 1.66707072  2.43454525 -1.50667653  1.66659174  2.29166158 -1.50681196]\n",
      "  output: [ 0.70570663  2.04736287 -4.12867523]\n",
      "10000 iterations took 2.7 seconds\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "    \n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    # define the terms in our gradient descent equations\n",
    "    w = weights_output\n",
    "    v = weights_hidden\n",
    "    yhat = linear(x, v, bias)\n",
    "    g = sigmoid(yhat)\n",
    "    y = linear(g, w, bias)\n",
    "    f = sigmoid(y)\n",
    "    \n",
    "    # calculate the mean-squared error for the weights we have\n",
    "    error = numpy.power(f - ground_truth, 2).mean()\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error))\n",
    "    \n",
    "    # calculate out partial derivatives for the output layer's weights\n",
    "    dE_df = 2.0 * (f - ground_truth) / f.size\n",
    "    df_dy = f * (1.0 - f)\n",
    "    dy_dw = g\n",
    "    dE_dy = dE_df * df_dy\n",
    "    dE_dw = numpy.dot(dy_dw.T, dE_dy)\n",
    "    \n",
    "    # calculate partial derivatives with respect to hidden layer weights\n",
    "    dy_dg = w\n",
    "    dE_dg = numpy.dot(dE_dy, dy_dg.T)\n",
    "    dg_dyhat = g * (1.0 - g)\n",
    "    dyhat_dv = x\n",
    "    dE_dyhat = dE_dg * dg_dyhat\n",
    "    dE_dv = numpy.dot(dyhat_dv.T, dE_dyhat)\n",
    "    \n",
    "    # update weights and biases - the error is the sum of error over each input\n",
    "    weights_hidden -= LEARNING_RATE * dE_dv\n",
    "    weights_output -= LEARNING_RATE * dE_dw\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"  hidden: {}\".format(weights_hidden.flatten()))\n",
    "print(\"  output: {}\".format(weights_output.flatten()))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of weights $\\mathbf{v}$ for our hidden layer and $\\mathbf{w}$ for our output layer.  To make predictions using these weights, we just run our inputs $\\mathbf{x}$ through our trained model $ \\mathbf{f}(\\mathbf{y}(\\mathbf{g}(\\mathbf{\\hat{y}}(\\mathbf{x})))) $.\n",
    "\n",
    "### Predicting outputs\n",
    "\n",
    "Since this multilayer perceptron has two layers, we have to\n",
    "\n",
    "1. Calculate our hidden layer's linear model output, $\\hat{y}$, given our input\n",
    "2. Take the output of $\\hat{y}$ and feed it into the hidden layer's activation function, $g(\\hat{y})$\n",
    "3. Take the output of $g(\\hat{y})$ and feed it into the output layer's linear model $y(g)$\n",
    "4. Take the output of $y(g)$ and feed it into the output layer's activation function to get our final prediction, $f(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vs. actual outputs:\n",
      "             input1  input2  output  prediction     error\n",
      "observation                                              \n",
      "1                 0       0       0    0.334522 -0.334522\n",
      "2                 0       1       1    0.846053  0.153947\n",
      "3                 1       0       1    0.849022  0.150978\n",
      "4                 1       1       1    0.925358  0.074642\n"
     ]
    }
   ],
   "source": [
    "# traverse the hidden layer\n",
    "predicted_output = linear(inputs, weights_hidden, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "\n",
    "# traverse the output layer\n",
    "predicted_output = linear(predicted_output, weights_output, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "\n",
    "# convert output to dataframe\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "print(\"Predicted vs. actual outputs:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scrutinize the predictions and error from this multilayer model, you'll see that it did a lot worse than our simple perceptron; this is due in part to\n",
    "\n",
    "1. we are now trying to find the optimal values of _five_ weights instead of two in the same number of iterations of gradient descent\n",
    "2. we are using a more complicated model to predict the behavior of something that is fundamentally not that complicated\n",
    "\n",
    "This speaks to the importance of understanding how to match your neural network to the problem it is trying to model.  Although you definitely don't want to underfit the model to the complexity of your inputs and outputs, you also don't want to overfit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with PyTorch\n",
    "\n",
    "Now we implement the same multilayer perceptron using PyTorch.  This should look familiar; we're just adding two more layers to our sequential model container to reflect the new hidden layer and its activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inputs.shape[1], NUM_NEURONS_HIDDEN, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(NUM_NEURONS_HIDDEN, 1, bias=False),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "Parameter containing:\n",
      "tensor([[4.1702e-01, 3.0233e-01],\n",
      "        [7.2032e-01, 1.4676e-01],\n",
      "        [1.1437e-04, 9.2339e-02]], requires_grad=True)\n",
      "\n",
      "Starting output layer weights:\n",
      "Parameter containing:\n",
      "tensor([[0.1863, 0.3456, 0.3968]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1) # use same initial seed as before\n",
    "\n",
    "with torch.no_grad():\n",
    "    model[0].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN).astype(numpy.float32).T))\n",
    "    # model[0].bias = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, 1)))\n",
    "    model[2].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(NUM_NEURONS_HIDDEN, 1).astype(numpy.float32).T))\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(model[0].weight))\n",
    "print(\"Starting output layer weights:\\n{}\".format(model[2].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as PyTorch does, expressing the gradient descent process using PyTorch is much more straightforward since it automatically calculates our gradients for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651661e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350532e-02\n",
      "error at step  9000: 6.205267e-02\n",
      "Final weights:\n",
      "  hidden: [ 1.6670684  1.6665915  2.4345472  2.2916615 -1.5066769 -1.5068121]\n",
      "  output: [ 0.70570666  2.0473638  -4.128679  ]\n",
      "10000 iterations took 10.4 seconds\n"
     ]
    }
   ],
   "source": [
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "truth_tensor = torch.from_numpy(ground_truth.reshape(-1, 1))\n",
    "\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    f = model(inputs_tensor)\n",
    "\n",
    "    error = loss(f, truth_tensor)\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    error.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"  hidden: {}\".format(next(model[0].parameters()).detach().numpy().flatten()))\n",
    "print(\"  output: {}\".format(next(model[2].parameters()).detach().numpy().flatten()))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, prediction is equally straightforward.  We switch our model from training mode to evaluation mode, then run our inputs through it to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vs. actual outputs:\n",
      "             input1  input2  output  prediction     error\n",
      "observation                                              \n",
      "1                 0       0       0    0.334522 -0.334522\n",
      "2                 0       1       1    0.846053  0.153947\n",
      "3                 1       0       1    0.849022  0.150978\n",
      "4                 1       1       1    0.925358  0.074642\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted_output = model(inputs_tensor).detach().numpy()\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "print(\"Predicted vs. actual outputs:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
